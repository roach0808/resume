Jayson Whelpley
Senior AI & ML Engineer, System Designer	SP, Brazil
nanoweb.7jw@gmail.com 
www.linkedin.com/in/jayson-whelpley-489974362
https://jayson-seven.vercel.app/
   I am a Senior AI Engineer, I started my career in Machine Learning and Data Science about 10 years ago.
I have led various AI/ML projects in diverse industry sectors such as AdTech, Market Research, Financial Advising, Cloud Management, online retail, marketing, credit score modeling, data storage, healthcare and energy valuation. 
Recently, I was a Machine Learning Tech Lead at XnGen AI and Insoftai on the automation AI Agent building platform.
I am now focusing on a more entrepreneurial journey where I build tech businesses and write about my experience.
- Technical Proficiencies
Programming Languages: 	Python, Java, Rust, JavaScript/TypeScript, C++
Gen AI/ML Frameworks:	PyTorch, TensorFlow, sklearn, JAX, Keras, numpy, scipy, XGBoost, sktime, LightGBM, 					tinygrad, micrograd, CatBoost, langchain, Llama-index, Haystack, langGraph, AutoGen,
Crew AI, Hugging Face Transformer, GraphRAG(Neo4J), KAG
ML Architecture: 		FTI pipeline Architecture, Online Real-time/Asynchronous/Batch ML serving architecture, 4-				stage recommend architecture, Two-tower model(flexible neural network design)
MLOps:			AWS, GCP, Azure, W&B, MLFlow, Comet ML, Databricks, snowflake, 							Hopsworks, Power BI
LLM:				OpenAI, Anthropic, Azure Openai, Mistral, Multi-modal LLM(TTS/STT/VST/AST), SDXL, 					Gemini, NLP models (BERT, BioBERT, GPT, LLaMA), Diffuser
Prompt Optimization: 		AutoPrompt, PromptFify, Opik(Prompt management), DSPY
Fine-tuning&Training: 	Transformers, Unsloth, LitGPT, FL(with ONNX Runtime)
Benchmarks:			lighteval, evals, ragas, Perforator
Data Science:			Pydantic, Pandas, Pyspark, Databricks, BigQuery 
Web Frameworks: 		FastAPI, Flask, Django, Node.js, React.JS, Next.JS
Database:			PostgreSQL, MongoDB, Aurora DB, DynamoDB, Redis, Qdrant, SnowFlake, , 
PGVector, Pinecone, Milvus, chromadb
CI/CD:			Git, GitLab, Jenkins, Docker, Kubernetes, CircleCI, Terraform, CDK,  Pulumi
Orchestrator:			Docker Swarm, ECS, K8s, Airflow, Kubeflow, ZenML, PipeDream
API Design Models: 		REST API, RPC, GraphQL
Fine Tuning: 			Unsloth, TRL, Axolotl, LitGPT, AWS, SageMaker, Comet ML, Transformers
ML Inference Optimization 	TorchServe, NVIDIA TensorRT-LLM/Triton Inference Server, llama.cpp, KV cache, vllm, 					LitServe, Kserve, Continuous batching, Speculative Decoding, FL
- Career Experience
XnGen AI, British Columbia, Canada
     Senior Machine Learning Engineer							               6/2024 – present
I led and developed powerful AI-Driven Platforms and ML projects, streaming business operations by integrating AI- driven systems capable of handling up to 90% of customer inquiries. 
Developed Support-nGen™, a proprietary system designed to enhance customer service by efficiently managing FAQs, support tickets, and complex queries.
Also, I developed a LLM Twin, an advanced AI character that emulates individual writing/coding styles, voices, and personalities to serve as an effective writing co-pilot, facilitating brand creation by automating the writing process, generating new creative ideas, and streamlining content creation.
? Implemented expertise in implementing a sequential request processing system with a strong emphasis on low latency,  adopting an online real-time inference deployment architecture to enhance overall performance and responsiveness.
? Designed cloud-service/microservice architecture by splitting the ML service into a REST API server for business logic and an optimized LLM microservice, leveraging powerful machines and various engines to enhance latency and memory usage, thereby facilitating quick adaptation of the infrastructure based on different LLM sizes.
? Demonstrated a comprehensive approach by integrating Graph RAG with Neo4j within the business microservice, incorporating advanced RAG techniques to optimize the pre-retrieval, retrieval, and post-retrieval steps, resulting in enhanced accuracy and improved response, implementing binay quantization solution improving RAG search to 40x faster.
? Implemented a highly efficient deployment strategy for the LLM microservice on AWS SageMaker, utilizing Hugging Face’s Deep Learning Containers (DLCs) to enhance model inference. This robust infrastructure supported scalable, secure, and efficient real-time predictions through critical components such as SageMaker endpoints, model configuration, and inference components. By leveraging the Text Generation Inference (TGI) engine, the system achieved superior computational efficiency via tensor and dynamic batching for leading open-source LLMs like Mistral, Llama, and Falcon, accomplished optimizing performance with flash-attention, minimizing model size through model parallelism and weight quantization, enhancing throughput with speculative decoding, continuous batching, accelerating weight loading using safetensors, and enabling real-time interactions via token streaming, culminating in a responsive and effective LLM serving solution and achieving speedups of 2-4x or more.
? Developed and implemented fine-tuning process with Unsloth, a high-performance library, utilizing custom kernels, accelerating training by 2-5x and significantly reducing memory usage by up to 80%. 
? Engineered a business microservice using FastAPI, initially deployed to AWS Elastic Kubernetes Service (EKS) or AWS Elastic Container Service (ECS), involving Dockerization of the application, pushing the Docker image to AWS ECR, and configuring the deployment, while also orchestrating ML pipelines using ZenML / Airflow, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.
? Utilized advanced profiling tools to identify costly lines of code and uncover performance blind spots in local programs and Kubernetes clusters running on Linux, successfully optimizing CPU, GPU, and I/O performance, which led to an estimated 20% reduction in infrastructure costs.
? By integrating Ragas’s strengths in production monitoring and LLM-assisted metrics with ARES’s configurable evaluation process and classifier-based assessments, enhanced evaluation capabilities, achieving quick iterations and in-depth, customized evaluations that significantly improve performance outcomes.
? Exhibited strong leadership abilities by mentoring junior staff, enhancing their communication skills, and encouraging professional development.
Insoftai, Florida, United State
     AI/MLOps Technical Lead & Senior AI/ML Engineer      				                8/2020 – 5/2024
I developed Sierra.ai which redefines how businesses interact with data by simplifying document management and information accessibility. Sierra’s customer-centric approach establishes it as a trusted and secure partner for businesses of all sizes looking to implement multi-agent systems with langchain, langGraph and langSmith for mornitoring.
Also, I engineered a real-time personalized recommender system for H&M fashion articles using the 4-stage recommender architecture and a two-tower model design architecture, leveraging the Hopsworks AI Lakehouse.
? I was responsible for the development of Sierra.ai, revolutionizing document management and information accessibility for businesses, resulting in a 30% increase in operational efficiency for clients, applied extensive knowledge in AI/ML research to design and implement advanced algorithms, enhancing the platform's capability to process and analyze complex data sets effectively.
? Led the formulation and execution of technical strategies that align with business goals, contributing to a 25% growth in user adoption rates over the past year, designed and optimized multi-AI agents capable of autonomous decision-making, which improved response times by 40% and reduced manual intervention needs.
?  Designed and implemented three core ML serving architectures: online real-time inference, asynchronous inference, and offline batch transform. Balanced trade-offs between low latency and high throughput to optimize user experience and meet deployment requirements for throughput, latency, data, and infrastructure.
? I designed and implemented a modular Python package that orchestrates the ML workflow into three fully automated real-time pipelines—feature, training, and inference—while reducing processing time by 62.5% preserving the accuracy or quality.
? By addressing the limitations of traditional RAG with KAG, achieved over 94% accuracy in popular science queries and 93% in interpreting medical indicators, showed similarly impressive results, with precision rates of 91.6% and recall rates of 71.8% — a significant improvement over traditional RAG methods.
? I adopted MLOps best practices, including Infrastructure as Code (IaC), CI/CD, monitoring, experiment tracking, and model registries, ensuring the system is reproducible, testable, and trackable and deployed a scalable and cost-effective asynchronous batch architecture on AWS ECS and SQS, dynamically scaling based on job volume and achieving a 52% reduction in AWS costs.
? Designed 4-stage architecture to build a system that can handle recommendations from a catalog of millions of
items and two-tower model, a flexible neural network design that creates embeddings for users and items and optimized deploying ML models using Auto scaling, model optimization/parallelism/quantization, implementing a strategy similar to what TikTok employs for short videos, which will be applied to H&M retail items.
? Enhanced recommender systems by integrating advanced evaluation metrics such as NDCG, Precision@K, Recall@K, and Mean Reciprocal Rank (MRR), providing nuanced insights into model performance and user relevance, ultimately improving user satisfaction and engagement.
? Deployed real-time recommendations using Hopsworks Serverless and KServe, a runtime engine for serving predictive and generative ML models on Kubernetes, which simplifies autoscaling, networking, health checks, and server configuration while providing advanced features like GPU autoscaling and canary rollouts; through KServe, I successfully implemented two distinct services— the query encoder service and the ranking service—resulting in improved model performance and responsiveness in production.
Integralis, Melbourne, Australia
AI Full Stack Engineer	11/2019 – 8/2020
I worked on a TTS and STT solution, exposing it as an API that accurately clones voices from a short audio clip, significantly enhancing user experience in voice synthesis applications, and built an ML system for forecasting hourly energy consumption levels across Denmark, improving predictive accuracy and operational planning.
? I built an inference pipeline in LangChain as a serverless RESTful API, enabling real-time financial question answering, leveraging Neo4j for graph-based RAG, significantly improving data retrieval speeds and accuracy,
? Extended Meta’s Llama 3 model with multimodal projector, allowing direct audio input for faster responses compared to traditional ASR-LLM combinations, enhancing system efficiency.
? Designed a real-time streaming pipeline for developing financial advisor, monitoring financial news, processing documents, and storing them in a vector database, enhancing data retrieval efficiency.
?Developed a serverless continuous training solution that fine-tunes an LLM on financial data, optimizing model performance through automatic tracking and registry saving.
?Built efficient batch prediction pipelines using Python, leveraging a Feature Store and GCS, orchestrated with Airflow, resulting in streamlined predictions and improved operational workflows.
StackStudio.digital, London, United Kingdom
Backend-heavy AI Full Stack Engineer                                                                                                     12/2018 – 10/2019
I developed various AI-driven applications, including a Q/A AI chatbot, achieving a user satisfaction rate of 90% among over 10,000 daily users, significantly enhancing user engagement.
? I led the development of multiple chatbot projects, ensuring scalability and efficient management using AWS ECS, which improved operational reliability.
? I improved a project’s lighthouse performance score from 21 to 91 by leveraging my expertise in Next.js and React, resulting in a better user experience.
Essence Tech Labs, Ontario, Canada
Frontend Developer	2/2016 – 9/2018
I converted 24 design mockups into highly-quality, pixel-perfect code using React.js, enhancing the visual consistency and user interface of applications.
? Assisted in product development, boosting customer satisfaction by 17% through upgrades to existing suites, significantly  improving user retention.
? Optimized a JavaScript function for complex mathematical operations, enhancing performance and efficiency through my strong mathematical background and JavaScript proficiency.
- Education
Bachelor’s Degree in Computer Science
Dalhousie University	(2012 – 2016)